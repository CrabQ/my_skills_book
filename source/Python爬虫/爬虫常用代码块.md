# 爬虫常用代码块

## 随机选择user-agent

```python
# 随机选择user-agent
with open(sys.path[0] + '/user-agents.txt', 'r', encoding = 'utf-8') as f:
    list_user_agents = f.readlines()
    user_agent = random.choice(list_user_agents).strip()
headers = {'user-agent':user_agent}
```

## 保存到文件

```python
def save_img(source, filename):
    """
    保存文章中的图片
    :param source: 图片文件
    :param filename: 保存的图片名
    """
    dir_save_img = sys.path[0] + '/ioz_ac_spider_result/img/'
    if not os.path.exists(dir_save_img):
        os.makedirs(dir_save_img)
    try:
        # 保存图片
        with open(dir_save_img + filename, 'wb') as f:
            f.write(source)
    except OSError as e:
        print('图片保存失败：' + filename +'\n{e}'.format(e = e))

def save_page(list_article,filename):
    """
    保存到文件
    :param list_article: 结果
    :param filename: 保存的文件名
    """
    dir_save_page = sys.path[0] + '/bio360_spider_result/'
    if not os.path.exists(dir_save_page):
        os.makedirs(dir_save_page)
    try:
        with open(dir_save_page + filename , 'w', encoding = 'utf-8') as f:
            for i in list_article:
                f.write(i)
    except  OSError as e:
        print('内容保存失败：' + filename + '\n{e}'.format(e = e))
```

## 获取文章及其中所有的图片

```python
    if response_article:
        list_source = ['中国植保网']
        # 判断文章是原创还是转载，如果是原创则进行爬取
        html_source_local = etree.HTML(response_article.text)
        judge_source = html_source_local.xpath('//span[@class="style2 style3"]/a')[0].text
        # print(judge_source)

        for i in list_source:
            if judge_source == i:
                print('原创文章：' + url_full)
                # 通过正则表达式获取文章中需要的内容
                pattren_article = re.compile(r'<font size="\+1">.*<td height="174" align="center">', re.S|re.I)
                source_article = pattren_article.search(response_article.text)
                # print(source_article)
                if source_article:
                    source_article = source_article.group()
                    # 获取文章中所有的图片url链接: http://www.bio360.net/storage/image/2018/08/FG3XNGQGmD2HxBMqFgNNmiuLNXjTWHU9cnblI8TV.png
                    pattern_img = re.compile(r'<img(.*?)\ssrc="(.*?)"', re.I)
                    findall_img = pattern_img.findall(source_article)
                    # print('findall_img:', type(findall_img), findall_img)

                    # judge_img_get:判断能否获取图片
                    judge_img_get = True
                    for kw in findall_img:
                        # kw[1]: http://www.bio360.net/storage/image/2018/08/FG3XNGQGmD2HxBMqFgNNmiuLNXjTWHU9cnblI8TV.png
                        # 判断图片URL是否需要组合
                        # print('kw[1]',kw[1])
                        pattern_judge_img = re.compile(r'http', re.I)
                        judge_img = pattern_judge_img.search(kw[1])
                        if judge_img:
                            url_full_img = kw[1]
                        else:
                            # 图片网址:url_full_img: http://www.bio360.net/storage/image/2018/08/FG3XNGQGmD2HxBMqFgNNmiuLNXjTWHU9cnblI8TV.png
                            url_full_img =  'http://www.zgzbao.com/' + kw[1]
                            # 图片保存名：dwNNY7cwzRcOcsjRwMFcceLF9qTvhyDP8HiHTgQc.png
                        # print('url_full_img:', type(url_full_img), url_full_img)
                        pattern_name_save_img = re.compile(r'.*\/(.*\.[jpbg][pmin]\w+)', re.I)
                        try:
                            name_save_img = pattern_name_save_img.search(kw[1]).group(1).replace(r'/','').replace(r'\\','').replace(':','').replace('*','').replace('"','').replace('<','').replace('>','').replace('|','').replace('?','')
                            # print('name_save_img:', type(name_save_img), name_save_img)
                            # 获取图片
                            response_img = requests.get(url_full_img, headers = headers).content
                            # 保存图片
                            save_img(response_img, name_save_img)
                        except:
                            print('图片网址有误:' + '\n' + url_full_img)
                            # 如果图片获取不到，则赋值为false
                            judge_img_get = False
                            break

                    # 如果获取得到图片，再进行下一步
                    if judge_img_get:
                        # 提取url中的154727作为文件名保存: http://www.bio360.net/article/154727
                        pattren_filename = re.compile(r'.*\/(.*)?', re.I)
                        filename = pattren_filename.search(url_full).group(1) + '.html'
                        filename = filename.replace(r'/','').replace(r'\\','').replace(':','').replace('*','').replace('"','').replace('<','').replace('>','').replace('|','').replace('?','')
                        # print(filename)

                        # 解析文章，提取有用的内容，剔除不需要的，返回内容列表
                        list_article = parse_page(source_article)
                        # 保存文章内容
                        save_page(list_article, filename)
                    else:
                        print('获取不到图片：' + url_full)
                else:
                    print('get_page content error:' + url_full)

```

## 获取文章内容写入为xml文件

```python
def parse_page(source_local):
    """
    提取文章内容
    :param source_local: 文章内容
    """
    # 需要的内容保存到列表里，写入为.xml文件
    list_article = []
    list_article.append('<!DOCTYPE html>\n' + '<html>\n' + '<head>\n' + '<meta charset="utf-8"/>\n')

    # 利用etree.HTML，将字符串解析为HTML文档
    html_source_local = etree.HTML(source_local)
    # print(type(html_source_local),html_source_local)

    # title_article: 第四届发育和疾病的表观遗传学上海国际研讨会在沪隆重开幕
    title_article = html_source_local.xpath('//div[@class = "center_title"]')[0].text
    title_article = '<title>' + title_article + '</title>\n' + '</head>\n'
    list_article.append(title_article)
    # print(type(title_article),title_article)

    # source_article：来源： 中科普瑞 / 作者：  2018-09-11
    source_article = html_source_local.xpath('//div[@class="item-time col-sm-8"]')[0].text
    pattern_search_source = re.compile(r'来源：(.*?)/{1}', re.I|re.S)
    result_source = pattern_search_source.search(source_article).group(1).strip()
    pattern_search_time = re.compile(r'\d\d\d\d-\d\d-\d\d', re.I|re.S)
    result_time = pattern_search_time.search(source_article).group().strip()
    pattern_search_user_ = re.compile(r'作者：(.*?)\d\d\d\d-\d\d-\d\d', re.I|re.S)
    result_user = pattern_search_user_.search(source_article).group(1).replace('/','').replace('时间：','').strip()
    source_article = '<body>\n' + '<div class = "source">' + result_source + '</div>\n' + '<div class = "user">' + result_user + '</div>\n' + '<div class = "time">' + result_time + '</div>\n' + '<content>\n'
    list_article.append(source_article)
    # print(type(source_article),source_article)

    # 通过正则表达式获取文章中需要的内容，即正文部分
    pattren_article_content = re.compile(r'<div id="nr">(.*)<script type="text/javascript">', re.I|re.S)
    source_article = pattren_article_content.search(source_local)

    if source_article:
        source_article = source_article.group(1)

        def img_url_name(match):
            """
            匹配文章内容中的图片url，替换为本地url
            """
            # http://www.bio360.net/storage/image/2018/08/FG3XNGQGmD2HxBMqFgNNmiuLNXjTWHU9cnblI8TV.png
            pattren_img_local = re.compile(r'\.[pjbg][pinm]', re.I)
            img_real_name = pattren_img_local.search(match.group())
            # print('match.group(1)', match.group())

            if img_real_name and match.group(1):
                pattern_kw_name_save_img = re.compile(r'.*\/(.*\.[jpbg][pmin]\w+)', re.I)
                kw_img_name = pattern_kw_name_save_img.search(match.group(1)).group(1).replace(r'/','').replace(r'\\','').replace(':','').replace('*','').replace('"','').replace('<','').replace('>','').replace('|','').replace('?','')
                img_name = '<img src="./img/' + kw_img_name + '" />'
                # print('img_name:', type(img_name), img_name)
                return img_name

        # 匹配文章内容中的图片url，替换为本地图片url
        pattren_img_local = re.compile(r'<img.*?\ssrc="(.*?)".*?>{1}', re.I|re.S)
        source_local = pattren_img_local.sub(img_url_name, source_article)

        # 剔除文章中不需要的内容
        def article_change(match):
            """
            匹配文章内容中的所有标签（a、img、p）除外，剔除掉
            """
            # <p src="./img/13SsuHuXECVJ<p style="text-align: center;"> p
            # print(match.group(),match.group(1))
            name_tag = ''
            return name_tag

        pattren_article_change = re.compile(r'<([^/aip]\w*)\s*.*?>{1}', re.I)
        source_local = pattren_article_change.sub(article_change, source_local)

        # 剔除所有除</ap>外的</>标签
        pattren_article_change_1 = re.compile(r'</[^pa].*?>{1}', re.I)
        source_local = pattren_article_change_1.sub('', source_local)

        # 剔除<P>标签的样式
        pattren_article_change_2 = re.compile(r'<p.*?>{1}', re.I)
        source_local = pattren_article_change_2.sub('<p>', source_local)

        # 剔除一些杂乱的样式
        # source_local = source_local.replace('&nbsp;','').replace('&','&amp;').strip()

        # 清洗后的正文
        print(source_local)
        source_local = source_local + '\n</content>\n' + '</body>\n' + '</html>\n'
        list_article.append(source_local)

    return list_article

```

## 保存到MySQL

```python
import pymysql
save_mysql(url_page, filename)


def save_mysql(url_source, url_local):
    """
    保存到文件
    :param url_source: 文章来源url
    :param url_local: 文章本地url
    """
    db = pymysql.connect(host='localhost', user='user', password='pwd', port=3306, db='user')
    cursor = db.cursor()
    url_local_full = '/home/bmnars/data/bio360_spider_result_v2/' + url_local
    update_time = time.strftime('%Y-%m-%d',time.localtime())
    data = {
        'source_url':url_source,
        'local_url':url_local_full,
        'source':'www.bio360.net',
        'update_time':update_time
    }
    table = '_cs_bmnars_link_v2'
    keys = ','.join(data.keys())
    values = ','.join(['%s']*len(data))
    sql = 'INSERT INTO {table}({keys}) VALUES ({values}) on duplicate key update '.format(table=table, keys=keys, values=values)
    update = ', '.join(['{key} = %s'.format(key=key) for key in data]) + ';'
    sql += update
    # print(sql)
    try:
        if cursor.execute(sql,tuple(data.values())*2):
            db.commit()
    except:
        print("save_mysql_failed:" + url_source)
        db.rollback()

    finally:
        cursor.close()
        db.close()
```
