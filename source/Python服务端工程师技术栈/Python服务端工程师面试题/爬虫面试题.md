# 爬虫面试题

在 requests 模块中,requests.content 和 requests.text 什么区别

```python
content存的是字节码 .text存的是.content编码后的字符串
```

简要写一下 lxml 模块的使用方法框架

```python
from lxml import etree

text = '''
<div>
    <ul>
    <li class="item-0"><a href="link1.html">first item</a></li>
    <li class="item-1"><a href="link2.html">second item</a></li>
    <li class="item-inactive"><a href="link3.html">third item</a></li>
    <li class="item-1"><a href="link4.html">fourth item</a></li>
    <li class="item-0"><a href="link5.html">fifth item</a>
     </ul>
 </div>
'''
html = etree.HTML(text)
result = html.xpath('//li')
print(result)
```

说一说 scrapy 的工作流程

```python
1.首先Spiders（爬虫）将需要发送请求的url(requests)经ScrapyEngine（引擎）交给Scheduler（调度器）
2.Scheduler（排序,入队）处理后,经ScrapyEngine,DownloaderMiddlewares(可选,主要有User_Agent, Proxy代理)交给Downloader
3.Downloader向互联网发送请求,并接收下载响应（response）.将响应（response）经ScrapyEngine,SpiderMiddlewares(可选)交给Spiders
4.Spiders处理response,提取数据并将数据经ScrapyEngine交给ItemPipeline保存（可以是本地,可以是数据库）
5.提取url重新经ScrapyEngine交给Scheduler进行下一个循环.直到无Url请求程序停止结束
```

scrapy 的去重原理

```python
1.Scrapy本身自带有一个中间件;
2.scrapy源码中可以找到一个dupefilters.py去重器;
3.需要将dont_filter设置为False开启去重,默认是False去重,改为True,就是没有开启去重；
4 .对于每一个url的请求,调度器都会根据请求得相关信息加密得到一个指纹信息,并且将指纹信息和set()集合中的指纹信息进行比对,如果set()集合中已经存在这个数据,就不在将这个Request放入队列中;5.如果set()集合中没有存在这个加密后的数据,就将这个Request对象放入队列中,等待被调度
```

scrapy 中间件有几种类,你用过哪些中间件

```python
spider中间件（主职过滤）对Request、Response的主要作用在过滤,可以对特定路径的URL请求丢弃、对特定页面响应过滤、同时对一些不含有指定信息的item过滤,当然pipeline也能实现item的过滤
下载中间件（主职加工）主要作用是加工,如给Request添加代理、添加UA、添加cookie,对Response返回数据编码解码、压缩解压缩、格式化等预处理.
用过user-agend中间件、代理ip中间件、selenium中间件、cookie中间件
```

你写爬虫的时候都遇到过什么反爬虫措施,你是怎么解决的

```python
反爬策略1:通过UA限制或者其他头信息限制
解决方案:构建用户代理池或其他头信息

反爬策略2:通过访问者IP限制
解决方案:构建IP代理池

反爬策略3:通过验证码限制
解决方案:手工打码、验证码接口自动识别或者通过机器学习自动识别

反爬策略4:通过数据的异步加载限制
解决方案:抓包分析或者使用PhantomJS

反爬策略5:通过Cookie限制
解决方案:进行Cookie处理

反爬策略6:通过JS限制（如请求的数据通过JS随机生成等）
解决方案:分析JS解密或者使用PhantomJS
```

为什么会用到代理

```python
匿名,防止被封
```

代理失效了怎么处理

```python
爬取之前先测试代理有效性,代理失效丢弃,换另一个.或者直接使用本机代理
```

列出你知道 header 的内容以及信息

```python
Accept:请求报头域,用于指定客户端可接受哪些类型的信息
Accept-Language:指定客户端可接受的语言类型
Accept-Encoding:指定客户端可接受的内容编码
Host:用于指定请求资源的主机 IP 和端口号,其内容为请求 URL 的原始服务器或网关的位置.从 HTTP 1.1 版本开始,请求必须包含此内容
Cookie:也常用复数形式 Cookies,这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据.它的主要功能是维持当前访问会话.例如,我们输入用户名和密码成功登录某个网站后,服务器会用会话保存登录状态信息,后面我们每次刷新或请求该站点的其他页面时,会发现都是登录状态,这就是 Cookies 的功劳.Cookies 里有信息标识了我们所对应的服务器的会话,每次浏览器在请求该站点的页面时,都会在请求头中加上 Cookies 并将其发送给服务器,服务器通过 Cookies 识别出是我们自己,并且查出当前状态是登录状态,所以返回结果就是登录之后才能看到的网页内容
Referer:此内容用来标识这个请求是从哪个页面发过来的,服务器可以拿到这一信息并做相应的处理,如做来源统计、防盗链处理等
User-Agent:简称 UA,它是一个特殊的字符串头,可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息.
Content-Type:也叫互联网媒体类型（Internet Media Type）或者 MIME 类型,在 HTTP 协议消息头中,它用来表示具体请求中的媒体类型信息.例如,text/html 代表 HTML 格式,image/gif 代表 GIF 图片,application/json 代表 JSON 类型
```

说一说打开浏览器访问 百度一下,你就知道 获取到结果,整个流程

```python
我们在浏览器中输入一个 URL,回车之后便会在浏览器中观察到页面内容.实际上,这个过程是浏览器向网站所在的服务器发送了一个请求,网站服务器接收到这个请求后进行处理和解析,然后返回对应的响应,接着传回给浏览器.响应里包含了页面的源代码等内容,浏览器再对其进行解析,便将网页呈现了出来
```

爬取速度过快出现了验证码怎么处理

```python
控制抓取
避免验证码:
1.控制抓取速度,定时或随机sleep
2.定时或定量切换ip地址
3.尝试其他途径获取,比如手机app

直面验证码:
将验证码返回打码,可采用人工或者打码平台
```

scrapy 和 scrapy-redis 有什么区别为什么选择 redis 数据库

```python
Scrapy 是一个通用的爬虫框架,但是不支持分布式,
Scrapy-redis是为了更方便地实现Scrapy分布式爬取,而提供了一些以redis为基础的组件(仅有组件)
```

分布式爬虫主要解决什么问题

```python
ip
带宽
cpu
io
```

写爬虫是用多进程好还是多线程好 为什么

```python
IO密集型代码(文件处理、网络爬虫等),多线程能够有效提升效率(单线程下有IO操作会进行IO等待,造成不必要的时间浪费,而开启多线程能在线程A等待时,自动切换到线程B,可以不浪费CPU的资源,从而能提升程序执行效率).在实际的数据采集过程中,既考虑网速和响应的问题,也需要考虑自身机器的硬件情况,来设置多进程或多线程
phantomjs 或者chrome-headless 来抓取的爬虫,应当是多进程的,因为每一个phan/chro 实例就是一个进程了,并发只能是多进程.此外爬虫中还是数据处理业务,如果数据处理业务是一个比较耗时的计算型操作,那么对数据处理部分应当设为多进程,但更多可能会考虑将该部分数据处理操作和爬虫程序解耦,也就是先把数据抓取下来,事后单独运行另外的程序解析数据
```

解析网页的解析器使用最多的是哪几个

```python
lxml、re、beautifulsope
```

需要登录的网页,如何解决同时限制 ip,cookie,session（其中有一些是动态生成的）在不使用动态爬取的情况下

```python
解决限制 IP 可以使用代理 IP 地址池、服务器；
不适用动态爬取的情况下可以使用反编译 JS 文件获取相应的文件,或者换用其他平台（比如手机端） 看看是否可以获取相应的 json 文件
```

验证码的解决（简单的:对图像做处理后可以得到的,困难的:验证码是点击,拖动等动态进行的）

```python
图形验证码:干扰、杂色不是特别多的图片可以使用开源库 Tesseract 进行识别,太过复杂的需要借助第三方打码平台
点击和拖动滑块验证码可以借助 selenium、无图形界面浏览器（chromedirver 或者 phantomjs） 和 pillow 包来模拟人的点击和滑动操作,pillow 可以根据色差识别需要滑动的位置
手动打码（有的验证码确实无解）
```
